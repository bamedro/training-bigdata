{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation de Hadoop"
      ],
      "metadata": {
        "id": "vMKY6LbQWzQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAbhTEJPiDis"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HADOOP_VERSION\"]=\"3.3.6\"\n",
        "!wget -nc https://dlcdn.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\n",
        "!tar -xzf hadoop-$HADOOP_VERSION.tar.gz\n",
        "!git clone https://github.com/bamedro/training-bigdata.git\n",
        "\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"]=\"/content/hadoop-\" + os.environ[\"HADOOP_VERSION\"]\n",
        "os.environ[\"PATH\"]=os.environ[\"PATH\"] + \":\" + os.environ[\"HADOOP_HOME\"]+\"/bin\"\n",
        "\n",
        "# On ajoute les chemins importants au fichier de configuration de Hadoop\n",
        "!sed -i \"s:# export JAVA_HOME=:export JAVA_HOME=$JAVA_HOME:g\" $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n",
        "!sed -i \"s:# export HADOOP_HOME=:export HADOOP_HOME=$HADOOP_HOME:g\" $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "# On vérifie que l'ajout s'est bien passé\n",
        "!cat $HADOOP_HOME/etc/hadoop/hadoop-env.sh | grep HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testons que l'installation fonctionne\n",
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROi5OCPrirue",
        "outputId": "8c05849b-6359-4cb8-de81-59b7c5df6759"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Premier test en mode \"Standalone\"\n",
        "\n",
        "Test d'Hadoop en version minimale, sans stockage distribué (HDFS) ni distribution (YARN)."
      ],
      "metadata": {
        "id": "WR7JcDN5Ytsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On lance un job Hadoop dont le rôle est de lister tous les termes commencant par 'dfs' parmis les fichiers de configuration XML d'Hadoop.\n",
        "!mkdir input\n",
        "!cp $HADOOP_HOME/etc/hadoop/*.xml input\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-$HADOOP_VERSION.jar grep input output 'dfs[a-z.]+'\n",
        "!cat output/*"
      ],
      "metadata": {
        "id": "DGkaiKV6kyyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation avec HDFS"
      ],
      "metadata": {
        "id": "Aay16yxzaM22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SSH est un pré-requis car les Hadoop utilise SSH pour lancer des commandes sur des noeuds distants.\n",
        "!sudo apt-get install openssh-server\n",
        "!sudo service ssh start\n",
        "\n",
        "# On doit créer une clé SSH par défaut et sans mot de passe\n",
        "!ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "# On initialise la première connexion\n",
        "!ssh -o StrictHostKeyChecking=no localhost hostname"
      ],
      "metadata": {
        "id": "Dc_OccgJzhYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On récupère les fichiers de configuration\n",
        "!cp /content/training-bigdata/1-initial-setup/core-site.xml /content/hadoop-$HADOOP_VERSION/etc/hadoop/\n",
        "!cp /content/training-bigdata/1-initial-setup/hdfs-site.xml /content/hadoop-$HADOOP_VERSION/etc/hadoop/\n",
        "\n",
        "# Regardez leur contenu\n",
        "!cat /content/hadoop-$HADOOP_VERSION/etc/hadoop/core-site.xml\n",
        "!cat /content/hadoop-$HADOOP_VERSION/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "PXZC1edpxBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut enfin initialiser notre 'nameNode' et démarrer le service qui va gérer le système de fichier distribué (DFS)\n",
        "import os\n",
        "os.environ[\"HDFS_NAMENODE_USER\"]=\"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"]=\"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"]=\"root\"\n",
        "\n",
        "!hdfs namenode -format -noninteractive\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnUFY_JZ1Fxk",
        "outputId": "c652d4d1-1887-4ce2-b708-57abafa22333"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "localhost: namenode is running as process 21172.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "Starting datanodes\n",
            "localhost: datanode is running as process 21282.  Stop it first and ensure /tmp/hadoop-root-datanode.pid file is empty before retry.\n",
            "Starting secondary namenodes [a9ecafc7324b]\n",
            "a9ecafc7324b: secondarynamenode is running as process 21504.  Stop it first and ensure /tmp/hadoop-root-secondarynamenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Faites quelques exercices pratiques à partir des exemples donnés dans le dossier 2-hdfs\n",
        "..."
      ],
      "metadata": {
        "id": "EpfdmsgkfZsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation avec YARN (sur un seul Node)"
      ],
      "metadata": {
        "id": "aGfNq3FygH6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On récupère les fichiers de configuration\n",
        "!cp /content/training-bigdata/1-initial-setup/mapred-site.xml /content/hadoop-$HADOOP_VERSION/etc/hadoop/\n",
        "!cp /content/training-bigdata/1-initial-setup/yarn-site.xml /content/hadoop-$HADOOP_VERSION/etc/hadoop/\n",
        "\n",
        "# N'hésitez pas à observer leur contenu\n",
        "!cat /content/hadoop-$HADOOP_VERSION/etc/hadoop/mapred-site.xml\n",
        "!cat /content/hadoop-$HADOOP_VERSION/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "3I-z735g5eNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut démarrer le service Yarn\n",
        "import os\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\"\n",
        "\n",
        "!$HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOT62a_s6Sc9",
        "outputId": "d6f12eed-5245-4a35-c9dd-42938d4c96ba"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting resourcemanager\n",
            "resourcemanager is running as process 17996.  Stop it first and ensure /tmp/hadoop-root-resourcemanager.pid file is empty before retry.\n",
            "Starting nodemanagers\n",
            "localhost: nodemanager is running as process 18101.  Stop it first and ensure /tmp/hadoop-root-nodemanager.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut lister les exemples de codes MapReduce embarqués dans la distribution Hadoop\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-$HADOOP_VERSION.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X85BzwLyfsL",
        "outputId": "93b6c74f-cd0f-4c39-cada-3640fb001769"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Déploiement d'un calcul pour estimer la valeur de Pi"
      ],
      "metadata": {
        "id": "jsfdiIClm0oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observer les arguments de la commande Pi\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi\n",
        "\n",
        "# Tester un découpage avec 5 Map, chacun effectuant 100 tirs aléatoires\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 5 100"
      ],
      "metadata": {
        "id": "w0UpNtdz3sFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Déployer un calcul à partir d'un code Python\n",
        "\n",
        "Deux fonctions, Map et Reduce, sont présentes dans le dossier training-bigdata/3-hadoop-streaming\n",
        "\n",
        "Étudiez leur fonctionnement"
      ],
      "metadata": {
        "id": "mJDN7EuLo5V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/training-bigdata/3-hadoop-streaming/mapper.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPzZj_vYA7W9",
        "outputId": "d96526e5-7f59-48d2-e8e1-7899631c5ec4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python3\n",
            "\n",
            "import sys\n",
            "\n",
            "# reading entire line from STDIN (standard input)\n",
            "for line in sys.stdin:\n",
            "  # to remove leading and trailing whitespace\n",
            "  line = line.strip()\n",
            "\n",
            "  # split the line into words\n",
            "  words = line.split()\n",
            "    \n",
            "  # we are looping over the words array and printing the word\n",
            "  # with the count of 1 to the STDOUT\n",
            "  for word in words:\n",
            "    # write the results to STDOUT (standard output);\n",
            "    # what we output here will be the input for the\n",
            "    # Reduce step, i.e. the input for reducer.py\n",
            "    print('%s\\t%s' % (word, 1))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/training-bigdata/3-hadoop-streaming/reducer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6-fAr8IBTvI",
        "outputId": "e85232ff-009d-4442-bcd0-1f7c3fed5a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python3\n",
            "\n",
            "import sys\n",
            "\n",
            "from operator import itemgetter\n",
            "import sys\n",
            "  \n",
            "current_word = None\n",
            "current_count = 0\n",
            "word = None\n",
            "  \n",
            "# read the entire line from STDIN\n",
            "for line in sys.stdin:\n",
            "  # remove leading and trailing whitespace\n",
            "  line = line.strip()\n",
            "  # splitting the data on the basis of tab we have provided in mapper.py\n",
            "  word, count = line.split('\\t', 1)\n",
            "  # convert count (currently a string) to int\n",
            "  try:\n",
            "    count = int(count)\n",
            "  except ValueError:\n",
            "    # count was not a number, so silently\n",
            "    # ignore/discard this line\n",
            "    continue\n",
            "  \n",
            "  # this IF-switch only works because Hadoop sorts map output\n",
            "  # by key (here: word) before it is passed to the reducer\n",
            "  if current_word == word:\n",
            "    current_count += count\n",
            "  else:\n",
            "    if current_word: #to not print current_word=None\n",
            "      # write result to STDOUT\n",
            "      print('%s\\t%s' % (current_word, current_count))\n",
            "    current_count = count\n",
            "    current_word = word\n",
            "  \n",
            "# do not forget to output the last word if needed!\n",
            "if current_word == word:\n",
            "  print('%s\\t%s' % (current_word, current_count))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/training-bigdata/3-hadoop-streaming/')\n",
        "!head ../LICENSE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WdEoLrUB1jF",
        "outputId": "28a794b4-b7e9-4d08-ed99-43217aad8247"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creative Commons Legal Code\n",
            "\n",
            "CC0 1.0 Universal\n",
            "\n",
            "    CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\n",
            "    LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\n",
            "    ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\n",
            "    INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\n",
            "    REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\n",
            "    PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head ../LICENSE | python mapper.py"
      ],
      "metadata": {
        "id": "BjarSfZyCE9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head ../LICENSE | python mapper.py | sort -k1,1"
      ],
      "metadata": {
        "id": "JC-wOg-bCaeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head ../LICENSE | python mapper.py | sort -k1,1 | python reducer.py"
      ],
      "metadata": {
        "id": "Ozd-027ICnLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}